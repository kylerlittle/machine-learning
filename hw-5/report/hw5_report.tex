\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage{float}

\author{Kyler Little\vspace{-0.6cm}}
\title{Homework \#5: Machine Learning\vspace{-0.3cm}}
\date{April 14, 2018\vspace{-0.7cm}}

\begin{document}
	\maketitle
	\section*{Problem \#1}
	Consider two finite-dimensional feature transforms $\Phi_1$ and $\Phi_2$ and their corresponding kernels $K_1$ and $K_2$. \\
	(a) Define $\Phi(x) = (\Phi_1(x), \Phi_2(x))$. Express the corresponding kernel of $\Phi$ in terms of $K_1$ and $K_2$. \\
	Firstly, it's important to note that $\Phi(x)$ is also finite-dimensional since both $\Phi_1(x)$ and $\Phi_2(x)$ are finite-dimensional. Next, it's a simple matter of linear algebra to derive the kernel. \\ $K(x,x')=\Phi(x)^T\Phi(x')=\left[\begin{array}{cc}\Phi_1(x)^T&\Phi_2(x)^T\end{array}\right]\left[\begin{array}{c}\Phi_1(x') \\ \Phi_2(x')\end{array}\right] = \Phi_1(x)^T \Phi_1(x') + \Phi_2(x)^T \Phi_2(x') = K_1(x,x')+K_2(x,x')$. 
	\\
	(b) Consider the matrix $\Phi_1(x) \Phi_2(x)^T$ and let $\Phi(x)$ be the vector representation of the matrix (say, by concatenating all the rows). Express the corresponding kernel of $\Phi$ in terms of $K_1$ and $K_2$. \\
	First, we need to find exactly what $\Phi$ is in this situation (i.e. what does it mean to concatenate rows). When we concatenate the rows, we simply put row 2 behind row 1, then row 3 behind row 2, and so on. We take the transpose of this vector so that it's a column vector. Now, let $u = \Phi_1$ and $v= \Phi_2$ to simplify the notation a bit. Also, assume $\Phi_1(x)$ has $n$ dimensions, and $\Phi_2(x)$ has $m$ dimensions. We have:
	\begin{align*}
		K(x,x') &= \Phi(x)^T\Phi(x') \\
		&= \text{concat}(\Phi_1(x) \Phi_2(x)^T)^T \text{concat}(\Phi_1(x') \Phi_2(x')^T) \\
		&= \text{concat}(uv^T)^T \text{concat}(u'v'^T) \\
		&= \sum_{i=1}^{m} \sum_{j=1}^{n} u_i v_j u'_i v'_j \\
		&= u^T u' v^T v' \\
		&= K_1(x,x') K_2(x,x')
	\end{align*}
	In this case, the kernel is simply the product of the other two kernels. \\
	(c) Hence, show that if $K_1$ and $K_2$ are kernels, then so are $K_1 + K_2$ and $K_1 K_2$. \\
	Assume otherwise. This means that it's not possible to find kernels $K_1+K_2$ and $K_1 K_2$ such that $K_1$ and $K_2$ are both separately kernels. However, we just did that in parts (a) and (b); thus, we have a contradiction. This means the original statement must be true. $\Box$
	
	\section*{Problem \#2}
	Exercise 8.16 (e-Chap:8-42) in LFD. Please ignore part (c) and only do (a), (b), (d). \\
	Exercise 8.16
	Show that the optimization problem in (8.30) is a QP-problem. \\
	(a) Show that the optimization variable is 
	$u =\left[\begin{array}{c}
		b\\
		w\\
		\xi
	\end{array}\right]$, where $\xi =\left[\begin{array}{c}
	\xi_1\\
	\vdots\\
	\xi_n
	\end{array}\right]$. \\
	(b) Show that $u^* \leftarrow \text{QP}(Q,p,A,c)$, where \\
	$Q =\left[\begin{array}{ccc}
	0&0_d^T&0_N^T\\
	0_d&I_d&0_{d \times N}\\
	0_N&0_{N \times d}&0_{N \times N}
	\end{array}\right]$, $p =\left[\begin{array}{c}
	0_{d+1} \\
	C \cdot 1_N
	\end{array} \right]$, $A =\left[\begin{array}{cc}
	YX&I_N\\
	0_{N \times d+1}&I_N
	\end{array}\right]$, and $c =\left[\begin{array}{c}
	1_N \\
	0_N
	\end{array} \right]$,\\
	and $YX$ is the signed data matrix from Exercise 8.4. \\
	(d) How do you determine which data points violate the margin, which data points are on the edge of the margin and which data points are correctly separated and outside the margin?
		
	\section*{Problem \#3}
	(a) Describe what are hinge loss, logistic regression loss, and 0-1 loss mathematically. Describe their similarities and differences using the unified picture we developed in class. \\
	(b) By relying on the result in the above question, consider a point that is correctly classified and distant from the decision boundary. Why would SVMâ€™s decision boundary be unaffected by this point, but the one learned by logistic regression be affected?
	
	\section*{Problem \#4}
	Summarize your observations from the coding portion into a short report. In your report, please report the accuracy result and total support vector number of each model. A briefly analysis based on the results is also needed.
	
	
\end{document}